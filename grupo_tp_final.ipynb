{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trabajo Práctico Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv\n",
        "from sklearn.datasets import load_iris, fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassEncoder:\n",
        "    def fit(self, y):\n",
        "        self.names = np.unique(y)\n",
        "        self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
        "        self.fmt = y.dtype\n",
        "\n",
        "    def _map_reshape(self, f, arr):\n",
        "        return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "\n",
        "    def transform(self, y):\n",
        "        return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "    def fit_transform(self, y):\n",
        "        self.fit(y)\n",
        "        return self.transform(y)\n",
        "\n",
        "    def detransform(self, y_hat):\n",
        "        return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "    def __init__(self):\n",
        "        self.encoder = ClassEncoder()\n",
        "\n",
        "    def _estimate_a_priori(self, y):\n",
        "        a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "        return np.log(a_priori)\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _predict_log_conditional(self, x, class_idx):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def fit(self, X, y, a_priori=None):\n",
        "        y = self.encoder.fit_transform(y)\n",
        "        self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "        assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "        self._fit_params(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        m_obs = X.shape[1]\n",
        "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "        for i in range(m_obs):\n",
        "            encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "            y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "        return y_hat.reshape(1,-1)\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        log_posteriori = [log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
        "                          in enumerate(self.log_a_priori)]\n",
        "        return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "    def _fit_params(self, X, y):\n",
        "        self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                         for idx in range(len(self.log_a_priori))]\n",
        "        self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "    def _predict_log_conditional(self, x, class_idx):\n",
        "        inv_cov = self.inv_covs[class_idx]\n",
        "        unbiased_x =  x - self.means[class_idx]\n",
        "        return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorizedQDA(QDA):\n",
        "    def _fit_params(self, X, y):\n",
        "        super()._fit_params(X,y)\n",
        "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
        "        self.tensor_means = np.stack(self.means)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_iris_dataset():\n",
        "    data = load_iris()\n",
        "    X_full = data.data\n",
        "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "    return X_full, y_full\n",
        "\n",
        "def get_penguins():\n",
        "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
        "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
        "    mask = df.isna().sum(axis=1) == 0\n",
        "    df = df[mask]\n",
        "    tgt = tgt[mask]\n",
        "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
        "\n",
        "def split_transpose(X, y, test_sz, random_state):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sz, random_state=random_state)\n",
        "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplo de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n",
            "(4, 90) (1, 90) (4, 60) (1, 60)\n",
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "# hiperparámetros\n",
        "rng_seed = 6543\n",
        "\n",
        "# Cargar y preparar los datos\n",
        "X_full, y_full = get_iris_dataset()\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
        "\n",
        "# Entrenar un modelo QDA\n",
        "qda = QDA()\n",
        "qda.fit(train_x, train_y)\n",
        "\n",
        "# Evaluar el modelo\n",
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplos de medición de tiempos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.02 ms ± 25.8 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\n",
        "qda.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.12 ms ± 5.93 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit\n",
        "\n",
        "model = QDA()\n",
        "model.fit(train_x, train_y)\n",
        "model.predict(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementación base\n",
        "1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n",
        "    1. Uniforme (cada clase tiene probabilidad 1/3)\n",
        "    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
        "2. Repetir el punto anterior para el dataset *penguin*.\n",
        "3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA (no múltiples prioris) ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?\n",
        "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?\n",
        "\n",
        "\n",
        "**Sugerencia:** puede resultar de utilidad para cada inciso de comparación utilizar tablas del siguiente estilo:\n",
        "\n",
        "<center>\n",
        "\n",
        "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
        ":---: | :---: | :---: | :---: | :---:\n",
        "QDA | Iris | 125 | 0.55 | 0.85\n",
        "LDA | Iris | 125 | 0.22 | 0.8\n",
        "\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Entrenar QDA sobre el dataset iris con diferentes distribuciones a priori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train error for iris dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0222\n",
            "Test error for the same dataset:  0.0167\n",
            "Train error for iris dataset and apriori probability distribution [0.05 0.05 0.9 ] is 0.0333\n",
            "Test error for the same dataset:  0.0500\n",
            "Train error for iris dataset and apriori probability distribution [0.05 0.9  0.05] is 0.0333\n",
            "Test error for the same dataset:  0.0000\n",
            "Train error for iris dataset and apriori probability distribution [0.9  0.05 0.05] is 0.0222\n",
            "Test error for the same dataset:  0.0167\n"
          ]
        }
      ],
      "source": [
        "# Se instancia QDA\n",
        "qda = QDA()\n",
        "\n",
        "# Se genera un array con las probabilidades de las distintas clases para el dataset iris\n",
        "probability_arrays = np.array([np.full(3,1/3),np.full(3,[.05,.05,.9]),np.full(3,[.05,.9,.05]),np.full(3,[.9,.05,.05])])\n",
        "for prob_array in probability_arrays:\n",
        "\n",
        "    qda.fit(train_x, train_y, prob_array)\n",
        "    train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "    test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "    print(f\"Train error for iris dataset and apriori probability distribution {prob_array} is {1 - train_acc:.4f}\")\n",
        "    print(f\"Test error for the same dataset:  {1 - test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Repetir el punto anterior para el dataset penguin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (342, 4), Y:(342, 1)\n",
            "(4, 205) (1, 205) (4, 137) (1, 137)\n",
            "Train error for penguin dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0098\n",
            "Test error for the same dataset:  0.0073\n",
            "Train error for penguin dataset and apriori probability distribution [0.05 0.05 0.9 ] is 0.0098\n",
            "Test error for the same dataset:  0.0073\n",
            "Train error for penguin dataset and apriori probability distribution [0.05 0.9  0.05] is 0.0098\n",
            "Test error for the same dataset:  0.0219\n",
            "Train error for penguin dataset and apriori probability distribution [0.9  0.05 0.05] is 0.0195\n",
            "Test error for the same dataset:  0.0219\n"
          ]
        }
      ],
      "source": [
        "# Código para el punto 2\n",
        "\n",
        "# Cargar y preparar los datos\n",
        "X_penguins_full, y_penguins_full = get_penguins()\n",
        "print(f\"X: {X_penguins_full.shape}, Y:{y_penguins_full.shape}\")\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins_full, y_penguins_full, 0.4, rng_seed)\n",
        "print(train_x_penguins.shape, train_y_penguins.shape, test_x_penguins.shape, test_y_penguins.shape)\n",
        "\n",
        "# Entrenar un modelo QDA\n",
        "qda_penguins = QDA()\n",
        "for prob_array in probability_arrays:\n",
        "\n",
        "    qda_penguins.fit(train_x_penguins, train_y_penguins,prob_array)\n",
        "    penguins_train_acc = accuracy(train_y_penguins, qda_penguins.predict(train_x_penguins))\n",
        "    penguins_test_acc = accuracy(test_y_penguins, qda_penguins.predict(test_x_penguins))\n",
        "    print(f\"Train error for penguin dataset and apriori probability distribution {prob_array} is {1 - penguins_train_acc:.4f}\")\n",
        "    print(f\"Test error for the same dataset:  {1 - penguins_test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Implementar LDA y comparar con QDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LDA for iris dataset and apriori probability distribution\n",
            "Train error for iris dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0222\n",
            "Test error for the same dataset:  0.0167\n",
            "Testing LDA for Penguin dataset\n",
            "Train error for Penguin dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0195\n",
            "Test error for the same dataset:  0.0219\n"
          ]
        }
      ],
      "source": [
        "# Código para el punto 3\n",
        "class LDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    cov_matrices = [np.cov(X[:,y.flatten()==idx], bias=True) for idx in range(len(self.log_a_priori))]\n",
        "    class_frequencies = np.bincount(y.flatten().astype(int)) / y.size\n",
        "\n",
        "    weighted_cov_mean = sum(cov_matrix * weight for cov_matrix, weight in zip(cov_matrices, class_frequencies))\n",
        "    self.inv_cov = inv(weighted_cov_mean)\n",
        "\n",
        "    self.means = np.array([X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))])\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_cov\n",
        "    half_unbiased_x =  x - 0.5*self.means[class_idx]\n",
        "    return self.means[class_idx].T @ inv_cov @ half_unbiased_x\n",
        "  \n",
        "#Prueba con Iris\n",
        "lda_iris = LDA()\n",
        "\n",
        "prob_array_iris = np.full(3,1/3)\n",
        "\n",
        "lda_iris.fit(train_x, train_y, prob_array)\n",
        "train_acc = accuracy(train_y, lda_iris.predict(train_x))\n",
        "test_acc = accuracy(test_y, lda_iris.predict(test_x))\n",
        "print(f\"Testing LDA for iris dataset and apriori probability distribution\")\n",
        "print(f\"Train error for iris dataset and apriori probability distribution {prob_array_iris} is {1 - train_acc:.4f}\")\n",
        "print(f\"Test error for the same dataset:  {1 - test_acc:.4f}\")\n",
        "\n",
        "#Prueba con dataset penguin\n",
        "lda_penguin = LDA()\n",
        "\n",
        "prob_array_penguin = np.full(3,1/3)\n",
        "\n",
        "lda_penguin.fit(train_x_penguins, train_y_penguins, prob_array)\n",
        "penguins_train_acc = accuracy(train_y_penguins, lda_penguin.predict(train_x_penguins))\n",
        "penguins_test_acc = accuracy(test_y_penguins, lda_penguin.predict(test_x_penguins))\n",
        "print(f\"Testing LDA for Penguin dataset\")\n",
        "print(f\"Train error for Penguin dataset and apriori probability distribution {prob_array_penguin} is {1 - penguins_train_acc:.4f}\")\n",
        "print(f\"Test error for the same dataset:  {1 - penguins_test_acc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Repetir la comparación con diferentes random seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Código para el punto 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Comparar tiempos de predicción entre QDA y TensorizedQDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Código para el punto 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimización matemática\n",
        "\n",
        "**Sugerencia:** considerar combinaciones adecuadas de `transpose`, `reshape` y, ocasionalmente, `flatten`. Explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
        "\n",
        "### QDA\n",
        "\n",
        "Debido a la forma cuadrática de QDA, no se puede predecir para *n* observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de *n x n* en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
        "\n",
        "1. Implementar el modelo `FasterQDA` (se recomienda heredarlo de TensorizedQDA) de manera de eliminar el ciclo for en el método predict.\n",
        "2. Comparar los tiempos de predicción de `FasterQDA` con `TensorizedQDA` y `QDA`.\n",
        "3. Mostrar (puede ser con un print) dónde aparece la mencionada matriz de *n x n*, donde *n* es la cantidad de observaciones a predecir.\n",
        "4. Demostrar que\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$ es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
        "5.Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente. ¿Hay cambios en los tiempos de predicción?\n",
        "\n",
        "\n",
        "### LDA\n",
        "\n",
        "1. \"Tensorizar\" el modelo LDA y comparar sus tiempos de predicción con el modelo antes implementado. *Notar que, en modo tensorizado, se puede directamente precomputar $\\mu^T \\cdot \\Sigma^{-1} \\in \\mathbb{R}^{k \\times 1 \\times p}$ y guardar eso en vez de $\\Sigma^{-1}$.*\n",
        "2. LDA no sufre del problema antes descrito de QDA debido a que no computa productos internos, por lo que no tiene un verdadero costo extra en memoria predecir \"en batch\". Implementar el modelo `FasterLDA` y comparar sus tiempos de predicción con las versiones anteriores de LDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "\n",
        "X_full, y_full = get_iris_dataset()\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "\n",
        "def compare_models(X_train, y_train, X_test, models, number=100):\n",
        "    results = []\n",
        "\n",
        "    # Ajustar y evaluar cada modelo\n",
        "    for model in models:\n",
        "        model_name = model.__class__.__name__\n",
        "        try:\n",
        "            # Ajustar el modelo\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Medir el tiempo de ejecución\n",
        "            predict_func = lambda: model.predict(X_test)\n",
        "            execution_time = timeit.timeit(predict_func, number=number) / number\n",
        "\n",
        "\n",
        "            results.append({\n",
        "                'Modelu': model_name,\n",
        "                'Tiempo promedio de ejecución: (s)': execution_time\n",
        "            })\n",
        "\n",
        "            print(f\"Modelo: {model_name}\")\n",
        "            print(f\"Tiempo promedio de ejecución: {execution_time:.6f} segundos\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error con {model_name}: {e}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### QDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Implementación de FasterQDA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FasterQDA(TensorizedQDA):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.show_n_by_n_matrix = False\n",
        "\n",
        "    def _predict_log_conditionals(self, x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        inner_prod = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov @ unbiased_x\n",
        "\n",
        "        if self.show_n_by_n_matrix:\n",
        "            self._show_n_by_n_matrix(inner_prod)\n",
        "\n",
        "\n",
        "        diag_inner_prod = np.stack([np.diagonal(inner_prod[i]) for i in range(inner_prod.shape[0])])\n",
        "        return 0.5 * np.log(det(self.tensor_inv_cov)).reshape(3, 1) - 0.5 * diag_inner_prod\n",
        "    \n",
        "    def _show_n_by_n_matrix(self, matrix):\n",
        "        print(f\"Matriz n x n (donde n es el número de observaciones a predecir):\")\n",
        "        print(f\"Forma de la matriz: {matrix.shape}\")\n",
        "        print(\"Primeras filas y columnas de la matriz:\")\n",
        "        print(matrix[0, :5, :5])  # Mostrar solo una parte de la matriz\n",
        "\n",
        "    def predict(self, X):\n",
        "        m_obs = X.shape[1]\n",
        "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "        stacked_X = np.stack(X)\n",
        "\n",
        "        encoded_y_hat_i = np.argmax(self.log_a_priori.reshape(3, 1) + self._predict_log_conditionals(stacked_X), axis=0)\n",
        "\n",
        "        y_hat = self.encoder.names[encoded_y_hat_i]\n",
        "\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Comparación de tiempos de predicción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo: QDA\n",
            "Tiempo promedio de ejecución: 0.001400 segundos\n",
            "--------------------------------------------------\n",
            "Modelo: TensorizedQDA\n",
            "Tiempo promedio de ejecución: 0.000582 segundos\n",
            "--------------------------------------------------\n",
            "Modelo: FasterQDA\n",
            "Tiempo promedio de ejecución: 0.000032 segundos\n",
            "--------------------------------------------------\n",
            "       Modelu  Tiempo promedio de ejecución: (s)\n",
            "          QDA                           0.001400\n",
            "TensorizedQDA                           0.000582\n",
            "    FasterQDA                           0.000032\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "models = [\n",
        "    QDA(),\n",
        "    TensorizedQDA(),\n",
        "    FasterQDA()\n",
        "]\n",
        "\n",
        "results = compare_models(train_x, train_y, test_x, models)\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Matriz n x n en FasterQDA:\n",
        "\n",
        "La matriz n x n aparece en el método `_predict_log_conditionals`:\n",
        "```python\n",
        "inner_prod = unbiased_x.transpose(0, 2, 1) @ self.tensor_inv_cov @ unbiased_x\n",
        "```\n",
        "\n",
        "Esta operación produce una matriz de forma (3, 60, 60), donde 60 es el número de observaciones a predecir.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz n x n (donde n es el número de observaciones a predecir):\n",
            "Forma de la matriz: (3, 60, 60)\n",
            "Primeras filas y columnas de la matriz:\n",
            "[[349.14286233 468.41721969 526.1539483   29.58564752 549.38070701]\n",
            " [468.41721969 634.30306746 709.04963714  42.20919219 740.77536697]\n",
            " [526.1539483  709.04963714 796.27377657  44.99957285 828.81320264]\n",
            " [ 29.58564752  42.20919219  44.99957285   6.52479153  50.03072051]\n",
            " [549.38070701 740.77536697 828.81320264  50.03072051 868.19357783]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['versicolor', 'versicolor', 'virginica', 'setosa', 'virginica',\n",
              "       'versicolor', 'versicolor', 'versicolor', 'versicolor',\n",
              "       'virginica', 'versicolor', 'versicolor', 'virginica', 'setosa',\n",
              "       'setosa', 'versicolor', 'setosa', 'setosa', 'versicolor', 'setosa',\n",
              "       'versicolor', 'setosa', 'virginica', 'setosa', 'virginica',\n",
              "       'setosa', 'versicolor', 'setosa', 'versicolor', 'virginica',\n",
              "       'versicolor', 'setosa', 'versicolor', 'virginica', 'virginica',\n",
              "       'versicolor', 'virginica', 'versicolor', 'setosa', 'virginica',\n",
              "       'setosa', 'virginica', 'setosa', 'setosa', 'versicolor',\n",
              "       'virginica', 'virginica', 'setosa', 'setosa', 'setosa',\n",
              "       'virginica', 'virginica', 'versicolor', 'setosa', 'virginica',\n",
              "       'setosa', 'versicolor', 'setosa', 'setosa', 'setosa'], dtype='<U10')"
            ]
          },
          "execution_count": 191,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fqda = FasterQDA()\n",
        "fqda.fit(train_x, train_y)\n",
        "\n",
        "fqda.show_n_by_n_matrix = True\n",
        "\n",
        "fqda.predict(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Demostración de la propiedad matricial\n",
        "\n",
        "Vamos a demostrar que:\n",
        "\n",
        "$$\n",
        "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
        "$$\n",
        "\n",
        "es decir, que se puede \"esquivar\" la matriz de *n x n* usando matrices de *n x p*.\n",
        "\n",
        "#### Paso 1: Expresión del producto matricial\n",
        "\n",
        "Si expresamos el producto matricial $A \\cdot B$ como una suma de multiplicaciones de fila por columna, podemos expresar elemento a elemento la matriz resultado de la siguiente manera:\n",
        "\n",
        "$$(A \\cdot B)_{ij} = \\sum_{k} A_{ik} B_{kj}$$\n",
        "\n",
        "#### Paso 2: Elementos de la diagonal\n",
        "\n",
        "Los elementos de la diagonal de la matriz resultado del producto pueden ser escritos de esta manera como:\n",
        "\n",
        "$$\\text{diag}(A \\cdot B) = \\left( \\sum_{k} A_{1k} B_{k1} , \\sum_{k} A_{2k} B_{k2} ,  \\ldots  , \\sum_{k} A_{nk} B_{kn} \\right)$$\n",
        "\n",
        "#### Paso 3: Producto elemento a elemento\n",
        "\n",
        "Ahora expresando el producto elemento a elemento de A y B transpuesta como:\n",
        "\n",
        "$$(A \\odot B^T)_{ij} = A_{ij} \\cdot B_{ji}$$\n",
        "\n",
        "#### Paso 4: Suma sobre las columnas\n",
        "\n",
        "El último paso es ver que sumando sobre todas las columnas (índice j) obtenemos cada elemento de la diagonal expresada antes, es decir:\n",
        "\n",
        "$$\\text{diag}(A \\cdot B) = \\sum_{\\text{cols}} A \\odot B^T$$\n",
        "\n",
        "Esta demostración prueba que podemos obtener la diagonal del producto de dos matrices $A \\cdot B$ sin necesidad de calcular explícitamente la matriz completa, lo cual es especialmente útil cuando trabajamos con matrices grandes, ya que nos permite \"esquivar\" la creación de una matriz *n x n* intermedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Demostración de la propiedad matricial:\n",
            "diag(A · B) = Σ_cols A ⊙ B^T = np.sum(A ⊙ B^T, axis=1)\n",
            "\n",
            "Forma de A: (5, 3)\n",
            "Forma de B: (3, 5)\n",
            "\n",
            "Resultado de diag(A · B):\n",
            "[0.56417179 0.41529267 1.68377588 0.5367293  0.22073285]\n",
            "\n",
            "Resultado de Σ_cols A ⊙ B^T:\n",
            "[0.56417179 0.41529267 1.68377588 0.5367293  0.22073285]\n",
            "\n",
            "Los resultados son iguales? Si\n",
            "\n",
            "Esto demuestra que podemos 'esquivar' la matriz n x n usando matrices de n x p.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 192,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def demonstrate_matrix_property(n=5, p=3):\n",
        "    # Crear matrices aleatorias A y B\n",
        "    A = np.random.rand(n, p)\n",
        "    B = np.random.rand(p, n)\n",
        "    \n",
        "    print(\"Demostración de la propiedad matricial:\")\n",
        "    print(f\"diag(A · B) = Σ_cols A ⊙ B^T = np.sum(A ⊙ B^T, axis=1)\")\n",
        "    \n",
        "    # Calcular diag(A · B)\n",
        "    diag_AB = np.diag(A @ B)\n",
        "    \n",
        "    # Calcular Σ_cols A ⊙ B^T\n",
        "    sum_product_product = np.sum(A * B.T, axis=1)\n",
        "    \n",
        "    print(\"\\nForma de A:\", A.shape)\n",
        "    print(\"Forma de B:\", B.shape)\n",
        "    print(\"\\nResultado de diag(A · B):\")\n",
        "    print(diag_AB)\n",
        "    print(\"\\nResultado de Σ_cols A ⊙ B^T:\")\n",
        "    print(sum_product_product)\n",
        "    \n",
        "    # Verificar si son iguales\n",
        "    are_equal = np.allclose(diag_AB, sum_product_product)\n",
        "    print(f\"\\nLos resultados son iguales? {'Si' if are_equal else 'No'}\")\n",
        "    \n",
        "    if are_equal:\n",
        "        print(\"\\nEsto demuestra que podemos 'esquivar' la matriz n x n usando matrices de n x p.\")\n",
        "    \n",
        "    return are_equal\n",
        "\n",
        "demonstrate_matrix_property()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Reimplementación eficiente de FasterQDA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientFasterQDA(FasterQDA):\n",
        "    def _predict_log_conditionals(self, x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        temp_product = self.tensor_inv_cov @ unbiased_x\n",
        "        unbiased_x_T = unbiased_x.transpose(0, 2, 1)\n",
        "        diag_inner_prod = np.sum(unbiased_x_T * temp_product.transpose(0, 2, 1), axis=2)\n",
        "\n",
        "        return 0.5 * np.log(det(self.tensor_inv_cov)).reshape(3, 1) - 0.5 * diag_inner_prod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparación de tiempos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo: QDA\n",
            "Tiempo promedio de ejecución: 0.001201 segundos\n",
            "--------------------------------------------------\n",
            "Modelo: TensorizedQDA\n",
            "Tiempo promedio de ejecución: 0.000539 segundos\n",
            "--------------------------------------------------\n",
            "Modelo: FasterQDA\n",
            "Tiempo promedio de ejecución: 0.000026 segundos\n",
            "--------------------------------------------------\n",
            "Modelo: EfficientFasterQDA\n",
            "Tiempo promedio de ejecución: 0.000016 segundos\n",
            "--------------------------------------------------\n",
            "            Modelu  Tiempo promedio de ejecución: (s)\n",
            "               QDA                           0.001201\n",
            "     TensorizedQDA                           0.000539\n",
            "         FasterQDA                           0.000026\n",
            "EfficientFasterQDA                           0.000016\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "models = [\n",
        "    QDA(),\n",
        "    TensorizedQDA(),\n",
        "    FasterQDA(),\n",
        "    EfficientFasterQDA()\n",
        "]\n",
        "\n",
        "results = compare_models(train_x, train_y, test_x, models)\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen de la Optimización de QDA\n",
        "\n",
        "En esta sección, implementamos y comparamos diferentes versiones de QDA:\n",
        "1. Creamos una clase FasterQDA que elimina el for en el método predict.\n",
        "2. Comparamos los tiempos de predicción de QDA, TensorizedQDA y FasterQDA.\n",
        "3. Identificamos la matriz n x n en FasterQDA y explicamos su importancia.\n",
        "4. Demostramos una propiedad matricial clave que nos permite evitar crear grandes matrices n x n.\n",
        "5. Implementamos una clase EfficientFasterQDA que utiliza esta propiedad para predicciones aún más rápidas.\n",
        "\n",
        "Los resultados muestran mejoras significativas en tiempo de ejecución desde QDA a TensorizedQDA a FasterQDA, y finalmente EfficientFasterQDA la implementación más rápida."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Tensorización de LDA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorizedLDA(LDA):\n",
        "    def _fit_params(self, X, y):\n",
        "        # Estimar matrices de covarianza para cada clase\n",
        "        cov_matrices = [np.cov(X[:, y.flatten()==idx], bias=True) for idx in range(len(self.log_a_priori))]\n",
        "        class_frequencies = np.bincount(y.flatten().astype(int)) / y.size\n",
        "\n",
        "        # Calcular el promedio ponderado de las matrices de covarianza\n",
        "        weighted_cov_sum = sum(cov_matrix * weight for cov_matrix, weight in zip(cov_matrices, class_frequencies))\n",
        "        self.inv_cov = inv(weighted_cov_sum)\n",
        "\n",
        "        # Calcular las medias para cada clase\n",
        "        self.means = np.array([X[:, y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                      for idx in range(len(self.log_a_priori))])\n",
        "        \n",
        "        # Precalcular μ^T · Σ^-1\n",
        "        self.precomputed_product = self.means.transpose(0, 2, 1) @ self.inv_cov\n",
        "\n",
        "    def _predict_log_conditionals(self, x):\n",
        "        half_unbiased_x = x - 0.5 * self.means\n",
        "        return self.precomputed_product @ half_unbiased_x\n",
        "\n",
        "    def predict(self, X):\n",
        "        m_obs = X.shape[1]\n",
        "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "\n",
        "        for i in range(m_obs):\n",
        "            encoded_y_hat_i = self._predict_one(X[:, i].reshape(-1, 1))\n",
        "            y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "\n",
        "        return y_hat.reshape(1, -1)\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x).flatten(), axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Implementación de FasterLDA:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FasterLDA(TensorizedLDA):\n",
        "    def predict(self, X):\n",
        "        m_obs = X.shape[1]\n",
        "        log_conditionals = self._predict_log_conditionals(X)\n",
        "        log_posteriors = self.log_a_priori.reshape(-1, 1) + log_conditionals\n",
        "        encoded_y_hat = np.argmax(log_posteriors, axis=0)\n",
        "        return self.encoder.detransform(encoded_y_hat.reshape(1, -1))\n",
        "\n",
        "    def _predict_log_conditionals(self, X):\n",
        "        # Calcular x - 0.5μ para todas las observaciones y clases\n",
        "        half_unbiased_X = X - 0.5 * self.means\n",
        "        # Aplicar el producto precalculado μ^T · Σ^-1\n",
        "        return self.precomputed_product @ half_unbiased_X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparación de tiempos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo: LDA\n",
            "Tiempo promedio de ejecución: 0.000755 segundos\n",
            "--------------------------------------------------\n",
            "Modelo: TensorizedLDA\n",
            "Tiempo promedio de ejecución: 0.000271 segundos\n",
            "--------------------------------------------------\n",
            "Modelo: FasterLDA\n",
            "Tiempo promedio de ejecución: 0.000062 segundos\n",
            "--------------------------------------------------\n",
            "       Modelu  Tiempo promedio de ejecución: (s)\n",
            "          LDA                           0.000755\n",
            "TensorizedLDA                           0.000271\n",
            "    FasterLDA                           0.000062\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "models = [\n",
        "    LDA(),\n",
        "    TensorizedLDA(),\n",
        "    FasterLDA()\n",
        "]\n",
        "\n",
        "results = compare_models(train_x, train_y, test_x, models)\n",
        "\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resumen de la Optimización de LDA\n",
        "\n",
        "Para LDA, seguimos un proceso de optimización similar:\n",
        "1. Creamos una clase TensorizedLDA que precalcula ciertos valores para una predicción más rápida.\n",
        "2. Implementamos una clase FasterLDA que optimiza aún más el proceso de predicción eliminando bucles.\n",
        "3. Comparamos los tiempos de predicción de LDA, TensorizedLDA y FasterLDA.\n",
        "\n",
        "Los resultados muestran que FasterLDA acelera significativamente el tiempo de ejecución en comparación con la implementación básica de LDA, mientras que TensorizedLDA ofrece una aceleración moderada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen General de las Optimizaciones\n",
        "\n",
        "En esta sección, exploramos varias técnicas de optimización para los clasificadores QDA y LDA:\n",
        "\n",
        "1. Tensorización: Al reestructurar nuestros datos y cálculos para usar tensores, pudimos aprovechar las operaciones eficientes de arrays de NumPy.\n",
        "2. Vectorización: Eliminamos los for en favor de operaciones vectorizadas, reduciendo significativamente el tiempo de cómputo.\n",
        "3. Precálculo: Al calcular y almacenar ciertos valores por adelantado, redujimos los cálculos redundantes durante la predicción.\n",
        "4. Optimizaciones matemáticas: Utilizamos propiedades matriciales para evitar crear grandes matrices intermedias, mejorando aún más la eficiencia.\n",
        "\n",
        "Estas técnicas resultaron en mejoras sustanciales en tiempo de ejecución tanto para QDA como para LDA, con las implementaciones más rápidas (EfficientFasterQDA y FasterLDA) mostrando mejoras de velocidad de un orden de magnitud en comparación con las implementaciones base.\n",
        "\n",
        "Las optimizaciones demuestran la importancia de considerar tanto los detalles algorítmicos como los de implementación, especialmente para conjuntos de datos grandes o aplicaciones sensibles al tiempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preguntas teóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Demostración de la función a maximizar en LDA\n",
        "\n",
        "2. Explicación de por qué QDA y LDA son \"quadratic\" y \"linear\"\n",
        "\n",
        "3. Diferencias entre la implementación de QDA y la descripción teórica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio teórico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cálculo de las derivadas de J respecto a cada parámetro de la red neuronal"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
