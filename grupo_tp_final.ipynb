{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trabajo Práctico Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv\n",
        "from sklearn.datasets import load_iris, fetch_openml\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassEncoder:\n",
        "    def fit(self, y):\n",
        "        self.names = np.unique(y)\n",
        "        self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
        "        self.fmt = y.dtype\n",
        "\n",
        "    def _map_reshape(self, f, arr):\n",
        "        return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "\n",
        "    def transform(self, y):\n",
        "        return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "    def fit_transform(self, y):\n",
        "        self.fit(y)\n",
        "        return self.transform(y)\n",
        "\n",
        "    def detransform(self, y_hat):\n",
        "        return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "    def __init__(self):\n",
        "        self.encoder = ClassEncoder()\n",
        "\n",
        "    def _estimate_a_priori(self, y):\n",
        "        a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "        return np.log(a_priori)\n",
        "\n",
        "    def _fit_params(self, X, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _predict_log_conditional(self, x, class_idx):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def fit(self, X, y, a_priori=None):\n",
        "        y = self.encoder.fit_transform(y)\n",
        "        self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "        assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "        self._fit_params(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        m_obs = X.shape[1]\n",
        "        y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "        for i in range(m_obs):\n",
        "            encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "            y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "        return y_hat.reshape(1,-1)\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        log_posteriori = [log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
        "                          in enumerate(self.log_a_priori)]\n",
        "        return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "    def _fit_params(self, X, y):\n",
        "        self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
        "                         for idx in range(len(self.log_a_priori))]\n",
        "        self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "\n",
        "    def _predict_log_conditional(self, x, class_idx):\n",
        "        inv_cov = self.inv_covs[class_idx]\n",
        "        unbiased_x =  x - self.means[class_idx]\n",
        "        return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TensorizedQDA(QDA):\n",
        "    def _fit_params(self, X, y):\n",
        "        super()._fit_params(X,y)\n",
        "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
        "        self.tensor_means = np.stack(self.means)\n",
        "\n",
        "    def _predict_log_conditionals(self,x):\n",
        "        unbiased_x = x - self.tensor_means\n",
        "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
        "        return 0.5*np.log(det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_iris_dataset():\n",
        "    data = load_iris()\n",
        "    X_full = data.data\n",
        "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "    return X_full, y_full\n",
        "\n",
        "def get_penguins():\n",
        "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
        "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
        "    mask = df.isna().sum(axis=1) == 0\n",
        "    df = df[mask]\n",
        "    tgt = tgt[mask]\n",
        "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
        "\n",
        "def split_transpose(X, y, test_sz, random_state):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_sz, random_state=random_state)\n",
        "    return X_train.T, y_train.T, X_test.T, y_test.T\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return (y_true == y_pred).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplo de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n",
            "(4, 90) (1, 90) (4, 60) (1, 60)\n",
            "Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "# hiperparámetros\n",
        "rng_seed = 6543\n",
        "\n",
        "# Cargar y preparar los datos\n",
        "X_full, y_full = get_iris_dataset()\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_x, train_y, test_x, test_y = split_transpose(X_full, y_full, 0.4, rng_seed)\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)\n",
        "\n",
        "# Entrenar un modelo QDA\n",
        "qda = QDA()\n",
        "qda.fit(train_x, train_y)\n",
        "\n",
        "# Evaluar el modelo\n",
        "train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "print(f\"Train (apparent) error is {1-train_acc:.4f} while test error is {1-test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplos de medición de tiempos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit\n",
        "\n",
        "qda.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit\n",
        "\n",
        "model = QDA()\n",
        "model.fit(train_x, train_y)\n",
        "model.predict(test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementación base\n",
        "1. Entrenar un modelo QDA sobre el dataset *iris* utilizando las distribuciones *a priori* a continuación ¿Se observan diferencias?¿Por qué cree? _Pista: comparar con las distribuciones del dataset completo, **sin splitear**_.\n",
        "    1. Uniforme (cada clase tiene probabilidad 1/3)\n",
        "    2. Una clase con probabilidad 0.9, las demás 0.05 (probar las 3 combinaciones)\n",
        "2. Repetir el punto anterior para el dataset *penguin*.\n",
        "3. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA (no múltiples prioris) ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "4. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Las conclusiones previas se mantienen?\n",
        "5. Estimar y comparar los tiempos de predicción de las clases `QDA` y `TensorizedQDA`. De haber diferencias ¿Cuáles pueden ser las causas?\n",
        "\n",
        "\n",
        "**Sugerencia:** puede resultar de utilidad para cada inciso de comparación utilizar tablas del siguiente estilo:\n",
        "\n",
        "<center>\n",
        "\n",
        "Modelo | Dataset | Seed | Error (train) | Error (test)\n",
        ":---: | :---: | :---: | :---: | :---:\n",
        "QDA | Iris | 125 | 0.55 | 0.85\n",
        "LDA | Iris | 125 | 0.22 | 0.8\n",
        "\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Entrenar QDA sobre el dataset iris con diferentes distribuciones a priori"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train error for iris dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0222\n",
            "Test error for the same dataset:  0.0167\n",
            "Train error for iris dataset and apriori probability distribution [0.05 0.05 0.9 ] is 0.0333\n",
            "Test error for the same dataset:  0.0500\n",
            "Train error for iris dataset and apriori probability distribution [0.05 0.9  0.05] is 0.0333\n",
            "Test error for the same dataset:  0.0000\n",
            "Train error for iris dataset and apriori probability distribution [0.9  0.05 0.05] is 0.0222\n",
            "Test error for the same dataset:  0.0167\n"
          ]
        }
      ],
      "source": [
        "# Se instancia QDA\n",
        "qda = QDA()\n",
        "\n",
        "# Se genera un array con las probabilidades de las distintas clases para el dataset iris\n",
        "probability_arrays = np.array([np.full(3,1/3),np.full(3,[.05,.05,.9]),np.full(3,[.05,.9,.05]),np.full(3,[.9,.05,.05])])\n",
        "for prob_array in probability_arrays:\n",
        "\n",
        "    qda.fit(train_x, train_y,prob_array)\n",
        "    train_acc = accuracy(train_y, qda.predict(train_x))\n",
        "    test_acc = accuracy(test_y, qda.predict(test_x))\n",
        "    print(f\"Train error for iris dataset and apriori probability distribution {prob_array} is {1 - train_acc:.4f}\")\n",
        "    print(f\"Test error for the same dataset:  {1 - test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Repetir el punto anterior para el dataset penguin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (342, 4), Y:(342, 1)\n",
            "(4, 205) (1, 205) (4, 137) (1, 137)\n",
            "Train error for penguin dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0098\n",
            "Test error for the same dataset:  0.0073\n",
            "Train error for penguin dataset and apriori probability distribution [0.05 0.05 0.9 ] is 0.0098\n",
            "Test error for the same dataset:  0.0073\n",
            "Train error for penguin dataset and apriori probability distribution [0.05 0.9  0.05] is 0.0098\n",
            "Test error for the same dataset:  0.0219\n",
            "Train error for penguin dataset and apriori probability distribution [0.9  0.05 0.05] is 0.0195\n",
            "Test error for the same dataset:  0.0219\n"
          ]
        }
      ],
      "source": [
        "# Código para el punto 2\n",
        "\n",
        "# Cargar y preparar los datos\n",
        "X_penguins_full, y_penguins_full = get_penguins()\n",
        "print(f\"X: {X_penguins_full.shape}, Y:{y_penguins_full.shape}\")\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "train_x_penguins, train_y_penguins, test_x_penguins, test_y_penguins = split_transpose(X_penguins_full, y_penguins_full, 0.4, rng_seed)\n",
        "print(train_x_penguins.shape, train_y_penguins.shape, test_x_penguins.shape, test_y_penguins.shape)\n",
        "\n",
        "# Entrenar un modelo QDA\n",
        "qda_penguins = QDA()\n",
        "for prob_array in probability_arrays:\n",
        "\n",
        "    qda_penguins.fit(train_x_penguins, train_y_penguins,prob_array)\n",
        "    penguins_train_acc = accuracy(train_y_penguins, qda_penguins.predict(train_x_penguins))\n",
        "    penguins_test_acc = accuracy(test_y_penguins, qda_penguins.predict(test_x_penguins))\n",
        "    print(f\"Train error for penguin dataset and apriori probability distribution {prob_array} is {1 - penguins_train_acc:.4f}\")\n",
        "    print(f\"Test error for the same dataset:  {1 - penguins_test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Implementar LDA y comparar con QDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LDA for iris dataset and apriori probability distribution\n",
            "Train error for iris dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0222\n",
            "Test error for the same dataset:  0.0167\n",
            "Testing LDA for Penguin dataset\n",
            "Train error for Penguin dataset and apriori probability distribution [0.33333333 0.33333333 0.33333333] is 0.0195\n",
            "Test error for the same dataset:  0.0219\n"
          ]
        }
      ],
      "source": [
        "# Código para el punto 3\n",
        "class LDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    cov_matrices = [np.cov(X[:,y.flatten()==idx], bias=True) for idx in range(len(self.log_a_priori))]\n",
        "    class_frequencies = np.bincount(y.flatten().astype(int)) / y.size\n",
        "\n",
        "    weighted_cov_mean = sum(cov_matrix * weight for cov_matrix, weight in zip(cov_matrices, class_frequencies))\n",
        "    self.inv_cov = inv(weighted_cov_mean)\n",
        "\n",
        "    self.means = np.array([X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
        "                  for idx in range(len(self.log_a_priori))])\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_cov\n",
        "    half_unbiased_x =  x - 0.5*self.means[class_idx]\n",
        "    return self.means[class_idx].T @ inv_cov @ half_unbiased_x\n",
        "  \n",
        "#Prueba con Iris\n",
        "lda_iris = LDA()\n",
        "\n",
        "prob_array_iris = np.full(3,1/3)\n",
        "\n",
        "lda_iris.fit(train_x, train_y, prob_array)\n",
        "train_acc = accuracy(train_y, lda_iris.predict(train_x))\n",
        "test_acc = accuracy(test_y, lda_iris.predict(test_x))\n",
        "print(f\"Testing LDA for iris dataset and apriori probability distribution\")\n",
        "print(f\"Train error for iris dataset and apriori probability distribution {prob_array_iris} is {1 - train_acc:.4f}\")\n",
        "print(f\"Test error for the same dataset:  {1 - test_acc:.4f}\")\n",
        "\n",
        "#Prueba con dataset penguin\n",
        "lda_penguin = LDA()\n",
        "\n",
        "prob_array_penguin = np.full(3,1/3)\n",
        "\n",
        "lda_penguin.fit(train_x_penguins, train_y_penguins, prob_array)\n",
        "penguins_train_acc = accuracy(train_y_penguins, lda_penguin.predict(train_x_penguins))\n",
        "penguins_test_acc = accuracy(test_y_penguins, lda_penguin.predict(test_x_penguins))\n",
        "print(f\"Testing LDA for Penguin dataset\")\n",
        "print(f\"Train error for Penguin dataset and apriori probability distribution {prob_array_penguin} is {1 - penguins_train_acc:.4f}\")\n",
        "print(f\"Test error for the same dataset:  {1 - penguins_test_acc:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Repetir la comparación con diferentes random seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Código para el punto 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Comparar tiempos de predicción entre QDA y TensorizedQDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Código para el punto 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimización matemática"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### QDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Código para la optimización de QDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Código para la optimización de LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preguntas teóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Demostración de la función a maximizar en LDA\n",
        "\n",
        "2. Explicación de por qué QDA y LDA son \"quadratic\" y \"linear\"\n",
        "\n",
        "3. Diferencias entre la implementación de QDA y la descripción teórica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejercicio teórico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cálculo de las derivadas de J respecto a cada parámetro de la red neuronal"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
